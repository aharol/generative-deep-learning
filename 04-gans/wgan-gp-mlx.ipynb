{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks With Gaussian Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools as ft\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx\n",
    "\n",
    "from wgan_gp.dataset import load_celeba\n",
    "from wgan_gp.utils import grid_image_from_batch, ensure_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "mx.random.seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "CRITIC_STEPS = 5\n",
    "GP_WEIGHT = 10.0\n",
    "Z_DIM = 100\n",
    "\n",
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "\n",
    "EXP_TIME = time.strftime(\"%I-%M%p_%B-%d-%Y\")\n",
    "SAVE_DIR = Path(\"./artifacts\") / EXP_TIME\n",
    "SAVE_EVERY_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_buf = load_celeba(split=\"train\")\n",
    "total_samples = len(data_buf)\n",
    "\n",
    "data = (\n",
    "    data_buf\n",
    "    .shuffle()\n",
    "    .to_stream()\n",
    "    .image_resize(\"image\", h=IMAGE_SHAPE[0], w=IMAGE_SHAPE[1])\n",
    "    .key_transform(\"image\", lambda x: (x.astype(\"float32\") - 127.5) / 127.5)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(prefetch_size=8, num_threads=8)\n",
    ")\n",
    "\n",
    "batch = next(data_iter)\n",
    "grid_image_from_batch(batch[\"image\"], num_rows=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_loss_fn(c_model, g_model, real_data):\n",
    "    \"\"\"Loss function for discriminator (i.e. critic)\"\"\"\n",
    "\n",
    "    batch_size = real_data.shape[0]\n",
    "\n",
    "    # generate latent variable\n",
    "    z = mx.random.normal(shape=(batch_size, Z_DIM))\n",
    "    fake_data = g_model(z)\n",
    "\n",
    "    # get discriminator predictions for real/fake data\n",
    "    real_pred = c_model(real_data)\n",
    "    fake_pred = c_model(fake_data)\n",
    "\n",
    "    # Compute losses\n",
    "    real_loss = real_pred.mean()  # Gradient ascent for real loss\n",
    "    fake_loss = fake_pred.mean()   # Gradient descent for fake loss\n",
    "    c_wass_loss = fake_loss - real_loss  # Wasserstein loss\n",
    "    c_gp = gradient_penalty(c_model, real_data, fake_data)\n",
    "    c_loss = c_wass_loss + GP_WEIGHT * c_gp\n",
    "\n",
    "    return c_loss, c_wass_loss, c_gp\n",
    "\n",
    "def gradient_penalty(c_model, real_data, fake_data):\n",
    "    \"\"\"Gradient penalty term\"\"\"\n",
    "    batch_size = real_data.shape[0]\n",
    "    # interpolate data\n",
    "    alpha = mx.random.normal(shape=(batch_size, 1, 1, 1))\n",
    "    interpo_data = (1 - alpha) * real_data + alpha * fake_data\n",
    "    grads = mx.grad(lambda x: c_model(x).sum())(interpo_data)\n",
    "    grad_norm = mx.linalg.norm(grads.reshape(batch_size, -1), axis=-1)\n",
    "    c_gp = mx.mean((grad_norm - 1.0) ** 2)\n",
    "    return c_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss_fn(c_model, g_model, batch_size):\n",
    "    \"\"\"Loss function for generator\"\"\"\n",
    "    # generate fake data\n",
    "    z = mx.random.normal(shape=(batch_size, Z_DIM))\n",
    "    # generate and classify fake data\n",
    "    fake_preds = c_model(g_model(z))\n",
    "    # obtain loss\n",
    "    g_loss = -fake_preds.mean()\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan_gp.models import Critic, Generator\n",
    "\n",
    "c_model = Critic(input_dim=3, output_dim=1)\n",
    "g_model = Generator(input_dim=Z_DIM, output_dim=3)\n",
    "\n",
    "mx.eval(c_model.parameters())\n",
    "mx.eval(g_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_optim=optim.AdamW(learning_rate=1e-4, betas=[0.9, 0.999], weight_decay=0.01)\n",
    "g_optim=optim.AdamW(learning_rate=1e-4, betas=[0.9, 0.999], weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, replace\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    c_loss: list[float] = field(default_factory=list)\n",
    "    g_loss: list[float] = field(default_factory=list)\n",
    "    c_wass_loss: list[float] = field(default_factory=list)\n",
    "    c_gp: list[float] = field(default_factory=list)\n",
    "    thrp: list[float] = field(default_factory=list)\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(c_model, c_loss_fn, c_optim,\n",
    "                g_model, g_loss_fn, g_optim,\n",
    "                data):\n",
    "\n",
    "    c_state = [\n",
    "        c_model.state, c_optim.state,\n",
    "        g_model.state, mx.random.state\n",
    "    ]\n",
    "\n",
    "    losses = {\"c_loss\":[], \"g_loss\":[], \"c_wass_loss\":[], \"c_gp\":[], \"thrp\":[]}\n",
    "\n",
    "    @ft.partial(mx.compile, inputs=c_state, outputs=c_state)\n",
    "    def train_critic(x):\n",
    "        loss_and_grad_fn_c = nn.value_and_grad(c_model, c_loss_fn)\n",
    "        (c_loss, c_wass_loss, c_gp), c_grads = loss_and_grad_fn_c(c_model, g_model, x)\n",
    "        c_optim.update(c_model, c_grads)\n",
    "        return c_loss, c_wass_loss, c_gp\n",
    "\n",
    "    g_state = [\n",
    "        c_model.state, c_optim.state,\n",
    "        g_model.state, g_optim.state,\n",
    "        mx.random.state\n",
    "    ]\n",
    "\n",
    "    @ft.partial(mx.compile, inputs=g_state, outputs=g_state)\n",
    "    def train_generator(batch_size):\n",
    "        loss_and_grad_fn_g = nn.value_and_grad(g_model, g_loss_fn)\n",
    "        g_loss, g_grads = loss_and_grad_fn_g(c_model, g_model, batch_size)\n",
    "        g_optim.update(g_model, g_grads)\n",
    "        return g_loss\n",
    "\n",
    "    with tqdm(total=total_samples) as pbar:\n",
    "\n",
    "        for batch_counter, batch in enumerate(data):\n",
    "\n",
    "            tic = time.perf_counter()\n",
    "            x = mx.array(batch[\"image\"])\n",
    "            c_loss, c_wass_loss, c_gp = train_critic(x)\n",
    "            mx.eval(c_state)\n",
    "            # Only update generator after running `num_critic_steps`\n",
    "            if batch_counter % CRITIC_STEPS == 0:\n",
    "                batch_size = x.shape[0]\n",
    "                g_loss = train_generator(batch_size)\n",
    "                mx.eval(g_state)\n",
    "            toc = time.perf_counter()\n",
    "            thrp = x.shape[0] / (toc - tic)\n",
    "\n",
    "            losses[\"c_loss\"].append(c_loss)\n",
    "            losses[\"g_loss\"].append(g_loss)\n",
    "            losses[\"c_wass_loss\"].append(c_wass_loss)\n",
    "            losses[\"c_gp\"].append(c_gp)\n",
    "            losses[\"thrp\"].append(thrp)\n",
    "\n",
    "            pbar.update(x.shape[0])\n",
    "\n",
    "    losses = {k: mx.array(v).mean() for k,v in losses.items()}\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    data.reset()\n",
    "\n",
    "    losses = train_epoch(\n",
    "        c_model, c_loss_fn, c_optim,\n",
    "        g_model, g_loss_fn, g_optim,\n",
    "        data, epoch)\n",
    "\n",
    "    for k, v in losses.items():\n",
    "        metrics.__dict__[k].append(v)\n",
    "\n",
    "    print(\"-\"*120)\n",
    "    print(\n",
    "        \" | \".join([\n",
    "            f\"Epoch: {epoch:02d}\",\n",
    "            f\"avg. Critic loss: {losses[\"c_loss\"]:.3f}\",\n",
    "            f\"avg. Generator loss: {losses[\"g_loss\"]:.3f}\",\n",
    "            f\"avg. Throughput: {losses[\"thrp\"]:.2f} images/second\",\n",
    "            f\"avg. Wasserstein loss: {losses[\"c_wass_loss\"]:.3f}\",\n",
    "            f\"avg. Gradient penalty: {losses[\"c_gp\"]:.3f}\",\n",
    "        ])\n",
    "    )\n",
    "    print(\"-\"*120)\n",
    "\n",
    "    # plot some samples after save_interval epochs\n",
    "    if epoch % SAVE_EVERY_EPOCH == 0:\n",
    "        z = mx.random.normal(shape=(BATCH_SIZE, Z_DIM))  # 128 random vectors\n",
    "        fake_images = mx.array(g_model(z))\n",
    "        img = grid_image_from_batch(fake_images, num_rows=8)\n",
    "        ensure_exists(SAVE_DIR / \"images\")\n",
    "        img.save(SAVE_DIR / \"gen_images\" / f\"image_{epoch}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Visualization\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,9))\n",
    "\n",
    "ax[0, 0].plot(range(1, NUM_EPOCHS+1), metrics.c_loss, 'r')\n",
    "ax[0, 0].title.set_text(\"c_loss\") #row=0, col=0\n",
    "ax[0, 0].xaxis.set_major_locator(MaxNLocator(integer=True))  # integer xaxis\n",
    "\n",
    "ax[0, 1].plot(range(1, NUM_EPOCHS+1), metrics.g_loss, 'b') #row=1, col=0\n",
    "ax[0, 1].title.set_text(\"g_loss\") #row=0, col=0\n",
    "ax[0, 1].xaxis.set_major_locator(MaxNLocator(integer=True)) # integer xaxis\n",
    "\n",
    "ax[1, 0].plot(range(1, NUM_EPOCHS+1), metrics.c_wass_loss, 'g') #row=0, col=1\n",
    "ax[1, 0].title.set_text(\"c_wass_loss\") #row=0, col=0\n",
    "ax[1, 0].xaxis.set_major_locator(MaxNLocator(integer=True)) # integer xaxis\n",
    "\n",
    "ax[1, 1].plot(range(1, NUM_EPOCHS+1), metrics.c_gp, 'm') #row=1, col=1\n",
    "ax[1, 1].title.set_text(\"c_gp\") #row=0, col=0\n",
    "ax[1, 1].xaxis.set_major_locator(MaxNLocator(integer=True)) # integer xaxis\n",
    "\n",
    "ensure_exists(SAVE_DIR / \"metrics\")\n",
    "fig.savefig(SAVE_DIR / \"metrics\" / \"training.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
