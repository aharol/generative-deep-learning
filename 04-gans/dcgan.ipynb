{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import zipfile\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx\n",
    "import mlx.data as dx\n",
    "from mlx.data.datasets.common import CACHE_DIR, ensure_exists, urlretrieve_with_progress\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "mx.random.seed(42)\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "Z_DIM = 100\n",
    "IMAGE_SHAPE = (64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_lego_bricks_wrapper(root=None):\n",
    "    base_url = \"https://www.kaggle.com/api/v1/datasets/download/joosthazelzet/lego-brick-images\"\n",
    "\n",
    "    if root is None:\n",
    "        root = CACHE_DIR / \"lego_brick_images\"\n",
    "    else:\n",
    "        root = Path(root)\n",
    "    ensure_exists(root)\n",
    "\n",
    "    pkl_file = root / \"lego_brick_images.pkl\"\n",
    "\n",
    "    def download():\n",
    "\n",
    "        temp_file = NamedTemporaryFile()\n",
    "        urlretrieve_with_progress(src=base_url, dst=temp_file.name)\n",
    "\n",
    "        temp_dir = TemporaryDirectory()\n",
    "        with zipfile.ZipFile(temp_file.name, 'r') as zf:\n",
    "            zf.extractall(temp_dir.name)\n",
    "            image_dir = Path(temp_dir.name) / \"dataset\"\n",
    "            images = [\n",
    "                {\"image\": Image.open(fname)\\\n",
    "                               .convert('L')\n",
    "                               .resize(size=(64, 64), resample=Image.BILINEAR)}\n",
    "                for fname in image_dir.glob(\"*.png\")\n",
    "            ]\n",
    "\n",
    "        with pkl_file.open(\"wb\") as f:\n",
    "            pickle.dump(images, f)\n",
    "\n",
    "    if not pkl_file.is_file():\n",
    "        download()\n",
    "\n",
    "    with pkl_file.open(\"rb\") as f:\n",
    "        return dx.buffer_from_vector(pickle.load(f))\n",
    "\n",
    "\n",
    "def load_lego_bricks(root=None):\n",
    "    \"\"\"Load a buffer with the Lego Bricks dataset.\n",
    "\n",
    "    If the data doesn't exist download it and save it for the next time.\n",
    "    Because the dataset is relatively small (40_000 samples), it is prepared\n",
    "    as a pickled Numpy array.\n",
    "\n",
    "    The dataset has no labels, and loaded as grayscale images of size 64x64\n",
    "\n",
    "    Args:\n",
    "        root (Path or str, optional): The directory to load/save the data. If\n",
    "            none is given the ``~/.cache/mlx.data/lego_brick_images`` is used.\n",
    "    \"\"\"\n",
    "\n",
    "    return _load_lego_bricks_wrapper(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lego_bricks(batch_size, root=None):\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x[..., None].astype(\"float32\") - 127.5) / 127.5\n",
    "\n",
    "    data = load_lego_bricks(root)\n",
    "\n",
    "    data_iter = (\n",
    "        data\n",
    "        .shuffle()\n",
    "        .to_stream()\n",
    "        .key_transform(\"image\", normalize)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(prefetch_size=8, num_threads=8)\n",
    "    )\n",
    "\n",
    "    return data_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_lego_bricks(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_image_from_batch(image_batch, num_rows):\n",
    "    \"\"\"\n",
    "    Generate a grid image from a batch of images.\n",
    "    Assumes input has shape (B, H, W, C).\n",
    "    \"\"\"\n",
    "\n",
    "    B, H, W, _ = image_batch.shape\n",
    "\n",
    "    num_cols = B // num_rows\n",
    "\n",
    "    # Calculate the size of the output grid image\n",
    "    grid_height = num_rows * H\n",
    "    grid_width = num_cols * W\n",
    "\n",
    "    # Normalize and convert to the desired data type\n",
    "    image_batch = np.array(image_batch * 255).astype(np.uint8)\n",
    "\n",
    "    # Reshape the batch of images into a 2D grid\n",
    "    grid_image = image_batch.reshape(num_rows, num_cols, H, W, -1)\n",
    "    grid_image = grid_image.swapaxes(1, 2)\n",
    "    grid_image = grid_image.reshape(grid_height, grid_width, -1)\n",
    "\n",
    "    # Convert the grid to a PIL Image\n",
    "    return Image.fromarray(grid_image.squeeze())\n",
    "\n",
    "samples = next(data)[\"image\"]\n",
    "grid_image_from_batch(samples, num_rows=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_dim, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=64, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=128, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=256, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=512, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Conv2d(in_channels=512, out_channels=output_dim, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            lambda x: x.reshape(x.shape[0], -1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: mx.array) -> mx.array:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_d = Discriminator(input_dim=1, output_dim=1)\n",
    "# mx.eval(model_d.parameters())\n",
    "# model_d(mx.ones(shape=(128, 64, 64, 1))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            lambda x: x.reshape(-1, 1, 1, input_dim),\n",
    "            nn.ConvTranspose2d(in_channels=input_dim, out_channels=512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm(num_features=512, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=256, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=128, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm(num_features=64, momentum=0.9),\n",
    "            ft.partial(nn.leaky_relu, negative_slope=0.2),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=output_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: mx.array) -> mx.array:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_g = Generator(input_dim=100, output_dim=1)\n",
    "# mx.eval(model_g.parameters())\n",
    "# model_g(mx.ones(shape=(128, 100))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_d(model_g, model_d, x, z_dim):\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    #  Discriminate fake images as fake\n",
    "    z = mx.random.normal(shape=(batch_size, z_dim))\n",
    "    fake_labels = mx.zeros(shape=(batch_size, 1))\n",
    "    fake_noisy_labels = fake_labels + 0.1 * mx.random.uniform(shape=(batch_size, 1))  # label smoothing\n",
    "    fake_loss = nn.losses.binary_cross_entropy(model_d(mx.stop_gradient(model_g(z))), fake_noisy_labels)\n",
    "\n",
    "    # Discriminate real images as real\n",
    "    real_labels = mx.ones(shape=(batch_size, 1))\n",
    "    real_noisy_labels = real_labels - 0.1 * mx.random.uniform(shape=(batch_size, 1))  # label smoothing\n",
    "    real_loss = nn.losses.binary_cross_entropy(model_d(x), real_noisy_labels)\n",
    "\n",
    "    return (fake_loss + real_loss) / 2.0\n",
    "\n",
    "\n",
    "def loss_fn_g(model_g, model_d, batch_size, z_dim):\n",
    "    \"\"\"Generator loss\"\"\"\n",
    "\n",
    "    # Classify generated images as real\n",
    "    z = mx.random.normal(shape=(batch_size, z_dim))\n",
    "    real_labels = mx.ones(shape=(batch_size, 1))\n",
    "    real_noisy_labels = real_labels - 0.1 * mx.random.uniform(shape=(batch_size, 1))  # label smoothing\n",
    "\n",
    "    return nn.losses.binary_cross_entropy(model_d(model_g(z)), real_noisy_labels) #, with_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = Discriminator(input_dim=IMAGE_SHAPE[-1], output_dim=1)\n",
    "model_g = Generator(input_dim=Z_DIM, output_dim=IMAGE_SHAPE[-1])\n",
    "\n",
    "mx.eval(model_d.parameters())\n",
    "mx.eval(model_g.parameters())\n",
    "\n",
    "optimizer_g = optim.AdamW(learning_rate=5e-5, betas=[0.5, 0.999], weight_decay=0.01)\n",
    "metrics_history = {\n",
    "    \"loss_d\": [],\n",
    "    \"loss_g\": [],\n",
    "    \"throughput\": [],\n",
    "}\n",
    "optimizer_d = optim.AdamW(learning_rate=1e-4, betas=[0.5, 0.999], weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model_d, model_g, optimizer_d, optimizer_g, data):\n",
    "\n",
    "    losses = {\"discriminator\": [], \"generator\": []}\n",
    "    samples_per_sec = []\n",
    "\n",
    "    state = [model_d.state, model_g.state, optimizer_d.state, optimizer_g.state]\n",
    "\n",
    "    @ft.partial(mx.compile, inputs=state, outputs=state)\n",
    "    def step(x):\n",
    "        loss_and_grad_fn_g = nn.value_and_grad(model_g, loss_fn_g)\n",
    "        loss_and_grad_fn_d = nn.value_and_grad(model_d, loss_fn_d)\n",
    "        # train discriminator\n",
    "        loss_d, grads_d = loss_and_grad_fn_d(model_g, model_d, x, Z_DIM)\n",
    "        optimizer_d.update(model_d, grads_d)\n",
    "        # train generator\n",
    "        loss_g, grads_g = loss_and_grad_fn_g(model_g, model_d, BATCH_SIZE, Z_DIM)\n",
    "        optimizer_g.update(model_g, grads_g)\n",
    "        return loss_d, loss_g\n",
    "\n",
    "    for batch_counter, batch in enumerate(data):\n",
    "\n",
    "        x = mx.array(batch[\"image\"])\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        loss_d, loss_g = step(x)\n",
    "        mx.eval(state)\n",
    "        toc = time.perf_counter()\n",
    "\n",
    "        losses[\"discriminator\"].append(loss_d.item())\n",
    "        losses[\"generator\"].append(loss_g.item())\n",
    "\n",
    "        throughput = x.shape[0] / (toc - tic)\n",
    "        samples_per_sec.append(throughput)\n",
    "        if batch_counter % 10 == 0:\n",
    "            print(\n",
    "                \" | \".join(\n",
    "                    (\n",
    "                        f\"Epoch {epoch:02d} [{batch_counter:03d}]\",\n",
    "                        f\"Discriminator Loss {loss_d:.3f}\",\n",
    "                        f\"Generator loss {loss_g:.3f}\",\n",
    "                        f\"Throughput: {throughput:.2f} images/second\",\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    mean_loss_d = mx.mean(mx.array(losses[\"discriminator\"]))\n",
    "    mean_loss_g = mx.mean(mx.array(losses[\"generator\"]))\n",
    "    samples_per_sec = mx.mean(mx.array(samples_per_sec))\n",
    "\n",
    "    return mean_loss_d, mean_loss_g, samples_per_sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_epochs = [1, 3, 5, 10, 20, 30, 40, 50]\n",
    "images_dir = Path(\"./images\")\n",
    "\n",
    "metrics_history = {\n",
    "    \"loss_d\": [],\n",
    "    \"loss_g\": [],\n",
    "    \"throughput\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    data.reset()\n",
    "\n",
    "    loss_d, loss_g, throughput = train_epoch(model_d, model_g, optimizer_d, optimizer_g, data)\n",
    "\n",
    "    metrics_history[\"loss_d\"].append(loss_d)\n",
    "    metrics_history[\"loss_g\"].append(loss_g)\n",
    "    metrics_history[\"throughput\"].append(throughput)\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            (\n",
    "                f\"Epoch: {epoch:02d}\",\n",
    "                f\"avg. Train loss discriminator {loss_d.item():.3f}\",\n",
    "                f\"avg. Train loss generator {loss_g.item():.3f}\",\n",
    "                f\"Throughput: {throughput.item():.2f} images/sec\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # plot some samples after save_interval epochs\n",
    "    if (epoch+1) in save_epochs:\n",
    "        z = mx.random.normal(shape=(BATCH_SIZE, Z_DIM))  # 128 random vectors\n",
    "        fake_images = mx.array(model_g(z))\n",
    "        img = grid_image_from_batch(fake_images, num_rows=8)\n",
    "        ensure_exists(images_dir)\n",
    "        img.save(f\"./images/image_{epoch + 1}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Visualization\n",
    "\n",
    "# Plot loss and accuracy in subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "ax1.plot(metrics_history[\"loss_d\"])\n",
    "ax2.plot(metrics_history[\"loss_g\"])\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
